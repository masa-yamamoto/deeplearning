{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chap11_homework.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_YdqQwi9DBm",
        "colab_type": "text"
      },
      "source": [
        "# 第11回講義 宿題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRGIUlLS9DBn",
        "colab_type": "text"
      },
      "source": [
        "## 課題. Deep Q-Network（DQN）でMountainCarを攻略せよ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yvIPBSj9DBo",
        "colab_type": "text"
      },
      "source": [
        "## Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8of0Ta69DBp",
        "colab_type": "text"
      },
      "source": [
        "Deep Q-Network（DQN）により、MountainCarを攻略してみましょう。  \n",
        "今回の評価は提出点のみとなりますが、より上手にゲームを攻略できるようチャレンジしてみてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xblg9_GJ9DBq",
        "colab_type": "text"
      },
      "source": [
        "## ルール"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPYyTfmP9DBq",
        "colab_type": "text"
      },
      "source": [
        "環境としてMountainCar-v0を利用します。  \n",
        "MountainCarは二つの山の間にある車を右の山の頂上まで運ぶゲームです。  \n",
        "エピソード終了時のRewardが-200よりも大きくなれば、成功となります。  \n",
        "- state: サイズ(2,)のnp.ndarray\n",
        "  - (車の位置, 車の速度)\n",
        "- action:\n",
        "    - 0: 車を左に移動させる\n",
        "    - 1: 車を移動させない\n",
        "    - 2: 車を右に移動させる\n",
        "- reward:\n",
        "    -1: エピソード終了まで\n",
        "- terminal:\n",
        "    - False: エピソード継続\n",
        "    - True: エピソード終了 (ゴールするか、200step経過)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kVJ353Z9DBr",
        "colab_type": "text"
      },
      "source": [
        "## 評価について"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFyELmZ59DBs",
        "colab_type": "text"
      },
      "source": [
        "- MountainCarをDQNによって攻略するコードを提出してください。(提出ページにそのままペーストして頂いて構いません)\n",
        "- なお今回の評価は提出点のみとなります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFpPtdPz9DBt",
        "colab_type": "text"
      },
      "source": [
        "## サンプルコード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yv5tQ2Su9DBt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "8908de09-82f8-4c1b-d967-9d453927b218"
      },
      "source": [
        "import os\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "#追加\n",
        "# import matplotlib\n",
        "# import matplotlib.animation as animation\n",
        "# import matplotlib.pyplot as plt\n",
        "# !apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "# !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "# !pip install JSAnimation\n",
        "# from pyvirtualdisplay import Display\n",
        "# pydisplay = Display(visible=0, size=(400, 300))\n",
        "# pydisplay.start()\n",
        "# from IPython import display\n",
        "# # 結果の確認\n",
        "# from JSAnimation.IPython_display import display_animation\n",
        "# from IPython.display import HTML\n",
        "# def animate(i):\n",
        "#     patch.set_data(frames[i])\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "env = gym.make('MountainCar-v0')\n",
        "\n",
        "tf.reset_default_graph()\n",
        "# WRITE ME\n",
        "\n",
        "n_states = 2\n",
        "n_actions = 3\n",
        "\n",
        "initializer = tf.variance_scaling_initializer()\n",
        "\n",
        "x_state = tf.placeholder(tf.float32, [None, n_states])\n",
        "\n",
        "def original_network(x):\n",
        "    with tf.variable_scope('Original', reuse=tf.AUTO_REUSE):\n",
        "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(x)\n",
        "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(h)\n",
        "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(h)\n",
        "        y = tf.layers.Dense(units=n_actions, kernel_initializer=initializer)(h)\n",
        "    return y\n",
        "\n",
        "def target_network(x):\n",
        "    with tf.variable_scope('Target', reuse=tf.AUTO_REUSE):\n",
        "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(x)\n",
        "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(h)\n",
        "        h = tf.layers.Dense(units=16, activation=tf.nn.elu, kernel_initializer=initializer)(h)\n",
        "        y = tf.layers.Dense(units=n_actions, kernel_initializer=initializer)(h)\n",
        "    return y\n",
        "\n",
        "q_original = original_network(x_state)\n",
        "vars_original = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Original')\n",
        "\n",
        "q_target = target_network(x_state)\n",
        "vars_target = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Target')\n",
        "\n",
        "copy_ops = [var_target.assign(var_original) for var_target, var_original in zip(vars_target, vars_original)]\n",
        "copy_weights = tf.group(*copy_ops)\n",
        "\n",
        "t = tf.placeholder(tf.float32, [None])\n",
        "x_action = tf.placeholder(tf.int32, [None])\n",
        "q_value = tf.reduce_sum(q_original * tf.one_hot(x_action, n_actions), axis=1)\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(tf.subtract(t,q_value)))\n",
        "optimizer = tf.train.AdamOptimizer()\n",
        "train_ops = optimizer.minimize(cost)\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, memory_size):\n",
        "        self.memory_size = memory_size\n",
        "        self.memory = deque([], maxlen = memory_size)\n",
        "    \n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        batch_indexes = np.random.randint(0, len(self.memory), size=batch_size).tolist()\n",
        "\n",
        "        state      = np.array([self.memory[index]['state'] for index in batch_indexes])\n",
        "        next_state = np.array([self.memory[index]['next_state'] for index in batch_indexes])\n",
        "        reward     = np.array([self.memory[index]['reward'] for index in batch_indexes])\n",
        "        action     = np.array([self.memory[index]['action'] for index in batch_indexes])\n",
        "        terminal   = np.array([self.memory[index]['terminal'] for index in batch_indexes])\n",
        "        \n",
        "        return {'state': state, 'next_state': next_state, 'reward': reward, 'action': action, 'terminal': terminal}\n",
        "\n",
        "memory_size = 50000 #メモリーサイズ\n",
        "initial_memory_size = 500 #事前に貯める経験数\n",
        "\n",
        "replay_memory = ReplayMemory(memory_size)\n",
        "\n",
        "step = 0\n",
        "\n",
        "while True:\n",
        "    state = env.reset()\n",
        "    terminal = False\n",
        "    \n",
        "    while not terminal:\n",
        "        action = env.action_space.sample() # ランダムに行動を選択\n",
        "        \n",
        "        next_state, reward, terminal, _ = env.step(action) # 状態、報酬、終了判定の取得\n",
        "        \n",
        "        transition = {\n",
        "            'state': state,\n",
        "            'next_state': next_state,\n",
        "            'reward': reward,\n",
        "            'action': action,\n",
        "            'terminal': int(terminal)\n",
        "        }\n",
        "        replay_memory.append(transition) # 経験の記憶\n",
        "\n",
        "        state = next_state\n",
        "        \n",
        "        step += 1\n",
        "    \n",
        "    if step >= initial_memory_size:\n",
        "        break\n",
        "        \n",
        "eps_start = 1.0\n",
        "eps_end = 0.01 #defaultは0.1\n",
        "n_steps = 10000\n",
        "def get_eps(step):\n",
        "    return max(0.01, (eps_end - eps_start) / n_steps * step + eps_start)\n",
        "  \n",
        "gamma = 0.95 #defaultは0.99\n",
        "target_update_interval = 1000 #重みの更新間隔\n",
        "batch_size = 32\n",
        "n_episodes = 300\n",
        "step = 0\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init.run()\n",
        "    copy_weights.run() # 初期重みのコピー\n",
        "    for episode in range(n_episodes):\n",
        "        state = env.reset()\n",
        "        terminal = False\n",
        "\n",
        "        total_reward = 0\n",
        "        total_q_max = []\n",
        "        while not terminal:\n",
        "            q = q_original.eval(feed_dict={x_state: state[None]}) # Q値の計算\n",
        "            total_q_max.append(np.max(q))\n",
        "\n",
        "            eps = get_eps(step) # εの更新\n",
        "            if np.random.random() < eps:\n",
        "                action = env.action_space.sample() # （ランダムに）行動を選択\n",
        "            else:\n",
        "                action = np.argmax(q) # 行動を選択\n",
        "            next_state, reward, terminal, _ = env.step(action) # 状態、報酬、終了判定の取得\n",
        "            reward = np.sign(reward)\n",
        "            total_reward += reward # エピソード内の報酬を更新\n",
        "\n",
        "            transition = {\n",
        "                'state': state,\n",
        "                'next_state': next_state,\n",
        "                'reward': reward,\n",
        "                'action': action,\n",
        "                'terminal': int(terminal)\n",
        "            }\n",
        "            replay_memory.append(transition) # 経験の記憶\n",
        "            \n",
        "            batch = replay_memory.sample(batch_size) # 経験のサンプリング\n",
        "            q_target_next = q_target.eval(feed_dict={x_state: batch['next_state']}) # ターゲットQ値の計算\n",
        "            t_value = batch['reward'] + (1 - batch['terminal']) * gamma * q_target_next.max(1)\n",
        "            \n",
        "            train_ops.run(feed_dict = {x_state: batch['state'], x_action: batch['action'], t: t_value}) # 訓練オペレーション\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if (step + 1) % target_update_interval == 0:\n",
        "                copy_weights.run() # 一定期間ごとに重みをコピー\n",
        "\n",
        "            step += 1\n",
        "\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            print('Episode: {}, Reward: {}, Q_max: {:.4f}, eps: {:.4f}'.format(episode + 1, total_reward, np.mean(total_q_max), eps))\n",
        "    \n",
        "    # 学習させたネットワークでTest\n",
        "#     frames = []\n",
        "    state = env.reset()\n",
        "    terminal = False\n",
        "\n",
        "    total_reward = 0\n",
        "    while not terminal:\n",
        "#         img = env.render(mode=\"rgb_array\")\n",
        "#         frames.append(img)\n",
        "\n",
        "        q = q_original.eval(feed_dict={x_state: state[None]})\n",
        "        action = np.argmax(q)\n",
        "\n",
        "        next_state, reward, terminal, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        state = next_state\n",
        "    \n",
        "    print('Test Reward:', total_reward)\n",
        "    \n",
        "# plt.figure(figsize=(frames[0].shape[1]/72.0, frames[0].shape[0]/72.0), dpi=72)\n",
        "# patch = plt.imshow(frames[0])\n",
        "# plt.axis('off')\n",
        "    \n",
        "# anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "# HTML(anim.to_jshtml())\n",
        "\n",
        "\n",
        "env.close()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 10, Reward: -200.0, Q_max: -1.9052, eps: 0.8021\n",
            "Episode: 20, Reward: -200.0, Q_max: -3.6088, eps: 0.6041\n",
            "Episode: 30, Reward: -200.0, Q_max: -5.0134, eps: 0.4061\n",
            "Episode: 40, Reward: -200.0, Q_max: -6.2918, eps: 0.2081\n",
            "Episode: 50, Reward: -200.0, Q_max: -7.5130, eps: 0.0126\n",
            "Episode: 60, Reward: -200.0, Q_max: -8.6419, eps: 0.0100\n",
            "Episode: 70, Reward: -140.0, Q_max: -9.1042, eps: 0.0100\n",
            "Episode: 80, Reward: -200.0, Q_max: -10.1651, eps: 0.0100\n",
            "Episode: 90, Reward: -200.0, Q_max: -10.9732, eps: 0.0100\n",
            "Episode: 100, Reward: -200.0, Q_max: -11.7054, eps: 0.0100\n",
            "Episode: 110, Reward: -200.0, Q_max: -12.2296, eps: 0.0100\n",
            "Episode: 120, Reward: -200.0, Q_max: -13.1268, eps: 0.0100\n",
            "Episode: 130, Reward: -200.0, Q_max: -13.4971, eps: 0.0100\n",
            "Episode: 140, Reward: -200.0, Q_max: -14.1151, eps: 0.0100\n",
            "Episode: 150, Reward: -200.0, Q_max: -14.0480, eps: 0.0100\n",
            "Episode: 160, Reward: -200.0, Q_max: -14.9404, eps: 0.0100\n",
            "Episode: 170, Reward: -200.0, Q_max: -15.0752, eps: 0.0100\n",
            "Episode: 180, Reward: -200.0, Q_max: -15.2391, eps: 0.0100\n",
            "Episode: 190, Reward: -200.0, Q_max: -15.3399, eps: 0.0100\n",
            "Episode: 200, Reward: -200.0, Q_max: -15.2079, eps: 0.0100\n",
            "Episode: 210, Reward: -126.0, Q_max: -14.3344, eps: 0.0100\n",
            "Episode: 220, Reward: -151.0, Q_max: -14.0343, eps: 0.0100\n",
            "Episode: 230, Reward: -200.0, Q_max: -15.9646, eps: 0.0100\n",
            "Episode: 240, Reward: -200.0, Q_max: -16.2877, eps: 0.0100\n",
            "Episode: 250, Reward: -200.0, Q_max: -16.4285, eps: 0.0100\n",
            "Episode: 260, Reward: -156.0, Q_max: -15.0567, eps: 0.0100\n",
            "Episode: 270, Reward: -165.0, Q_max: -14.7912, eps: 0.0100\n",
            "Episode: 280, Reward: -146.0, Q_max: -15.4690, eps: 0.0100\n",
            "Episode: 290, Reward: -114.0, Q_max: -14.7205, eps: 0.0100\n",
            "Episode: 300, Reward: -171.0, Q_max: -15.3241, eps: 0.0100\n",
            "Test Reward: -200.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI0tsfl3Fjdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}