{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4回講義 演習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import fetch_mldata, fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "課題 1. 多層パーセプトロンの実装と学習 (XOR)\n",
    "1. 活性化関数とその微分\n",
    "2. データセットの設定と重みの定義\n",
    "3. train関数とvalid関数\n",
    "4. 学習\n",
    "\n",
    "課題 2. 多層パーセプトロンの実装と学習 (MNIST)\n",
    "1. ソフトマックス関数\n",
    "2. データセットの設定\n",
    "3. 全結合層の定義\n",
    "4. train関数とvalid関数\n",
    "5. 学習\n",
    "\n",
    "課題 3. 数値微分 (勾配チェック)\n",
    "1. 1変数の場合\n",
    "2. 多変数の場合 (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題 1. 多層パーセプトロンの実装と学習 (XOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 活性化関数とその微分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\sigma (x) &= \\frac{1}{1 + \\exp(-x)} \\\\\n",
    "    \\sigma' (x) &= \\sigma(x) (1 - \\sigma(x))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.tanh(x * 0.5) * 0.5 + 0.5\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\mathrm{ReLU}(x) = \\max (0, x)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathrm{ReLU}'(x) =\n",
    "    \\begin{cases}\n",
    "    1 \\hspace{10mm} \\text{if} \\hspace{5mm} x > 0 \\\\\n",
    "    0 \\hspace{10mm} \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def deriv_relu(x):\n",
    "    return (x > 0).astype(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3. tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "    \\tanh(x) = \\frac{\\exp(2x) - 1}{\\exp(2x) + 1}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "    \\tanh'(x) = 1 - \\tanh^2(x)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def deriv_tanh(x):\n",
    "    return 1 - np.tanh(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. データセットの設定と重みの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XORデータセット\n",
    "x_train_xor = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
    "t_train_xor = np.array([[1], [1], [0], [0]])\n",
    "x_valid_xor, t_valid_xor = x_train_xor, t_train_xor\n",
    "\n",
    "# 重み (入力層の次元数: 2, 隠れ層の次元数: 8, 出力層の次元数: 1)\n",
    "W1 = np.random.uniform(low=-0.08, high=0.08, size=(2, 8)).astype('float32')\n",
    "b1 = np.zeros(8).astype('float32')\n",
    "W2 = np.random.uniform(low=-0.08, high=0.08, size=(8, 1)).astype('float32')\n",
    "b2 = np.zeros(1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. train関数とvalid関数\n",
    "隠れ層と出力層の2層からなるMLPを実装していきます。\n",
    "\n",
    "数式は以下のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 誤差関数\n",
    "負の対数尤度 (交差エントロピー)\n",
    "\n",
    "\\begin{align}\n",
    "    E({\\bf x}, {\\bf t}) = -\\frac{1}{N}\\sum^N_{i=1} \\left[{\\bf t}_i \\log {\\bf y}_i + (1 - {\\bf t}_i) \\log(1 - {\\bf y}_i)\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 順伝播\n",
    "\\begin{align*}\n",
    "    {\\bf u}^{(1)} &= {\\bf W}^{(1)\\mathrm{T}}{\\bf x} + {\\bf b}^{(1)} &\\text{(隠れ層)} \\\\\n",
    "    {\\bf h}^{(1)} &= \\mathrm{ReLU}({\\bf u}^{(1)}) &\\text{(隠れ層)} \\\\\n",
    "    {\\bf u}^{(2)} &= {\\bf W}^{(2)\\mathrm{T}}{\\bf h}^{(1)} + {\\bf b}^{(2)} &\\text{(出力層)} \\\\\n",
    "    {\\bf y} &= \\sigma({\\bf u}^{(2)}) &\\text{(出力層)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 逆伝播\n",
    "\\begin{align*}\n",
    "    {\\bf \\delta}^{(2)} &= {\\bf y} - {\\bf t} &\\text{(出力層)} \\\\\n",
    "    {\\bf \\delta}^{(1)} &= \\mathrm{ReLU}'({\\bf u}^{(1)}) \\odot ({\\bf W}^{(2)\\mathrm{T}}\\delta^{(2)}) &\\text{(隠れ層)}\n",
    "\\end{align*}\n",
    "\n",
    "#### 勾配の計算\n",
    "\\begin{align*}\n",
    "    \\nabla_{{\\bf W}^{(1)}}E &= \\frac{1}{N}\\delta^{(1)} {\\bf x}^{\\mathrm{T}} &\\text{(隠れ層)} \\\\\n",
    "    \\nabla_{{\\bf b}^{(1)}}E &= \\frac{1}{N}\\delta^{(1)} \\mathbb{1}_N &\\text{(隠れ層)} \\\\\n",
    "    \\nabla_{{\\bf W}^{(2)}}E &= \\frac{1}{N}\\delta^{(2)} {\\bf h}^{(1)\\mathrm{T}} &\\text{(出力層)} \\\\\n",
    "    \\nabla_{{\\bf b}^{(2)}}E &= \\frac{1}{N}\\delta^{(2)} \\mathbb{1}_N &\\text{(出力層)}\n",
    "\\end{align*}\n",
    "\n",
    "#### 重みの更新\n",
    "\\begin{align*}\n",
    "    {\\bf W}^{(1)} &\\leftarrow {\\bf W}^{(1)} - \\epsilon \\nabla_{{\\bf W}^{(1)}} E &\\text{(隠れ層)} \\\\\n",
    "    {\\bf b}^{(1)} &\\leftarrow {\\bf b}^{(1)} - \\epsilon \\nabla_{{\\bf b}^{(1)}} E &\\text{(隠れ層)} \\\\\n",
    "    {\\bf W}^{(2)} &\\leftarrow {\\bf W}^{(2)} - \\epsilon \\nabla_{{\\bf W}^{(2)}} E &\\text{(出力層)} \\\\\n",
    "    {\\bf b}^{(2)} &\\leftarrow {\\bf b}^{(2)} - \\epsilon \\nabla_{{\\bf b}^{(2)}} E &\\text{(出力層)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logの中身が 0になるのを防ぐ\n",
    "def np_log(x):\n",
    "    return np.log(np.clip(x, 1e-10, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xor(x, t, eps):\n",
    "    \"\"\"\n",
    "    :param x: np.ndarray, 入力データ, shape=(batch_size, 入力層の次元数)\n",
    "    :param t: np.ndarray, 教師ラベル, shape=(batch_size, 出力層の次元数)\n",
    "    :param eps: float, 学習率\n",
    "    \"\"\"\n",
    "    global W1, b1, W2, b2\n",
    "    \n",
    "    batch_size = x.shape[0]\n",
    "    \n",
    "    # 順伝播\n",
    "    u1 = np.matmul(x, W1) + b1 # shape: (batch_size, 隠れ層の次元数)\n",
    "    h1 = relu(u1)\n",
    "\n",
    "    u2 = np.matmul(h1, W2) + b2 # shape: (batch_size, 出力層の次元数)\n",
    "    y = sigmoid(u2)\n",
    "    \n",
    "    # 誤差の計算\n",
    "    cost = (- t * np_log(y) - (1 - t) * np_log(1 - y)).mean()\n",
    "    \n",
    "    # 逆伝播\n",
    "    delta_2 = y - t # shape: (batch_size, 出力層の次元数)\n",
    "    delta_1 = deriv_relu(u1) * np.matmul(delta_2, W2.T) # shape: (batch_size, 隠れ層の次元数)\n",
    "\n",
    "    # 勾配の計算\n",
    "    dW1 = np.matmul(x.T, delta_1) / batch_size # shape: (入力層の次元数, 隠れ層の次元数)\n",
    "    db1 = np.matmul(np.ones(batch_size), delta_1) / batch_size # shape: (隠れ層の次元数,)\n",
    "    \n",
    "    dW2 = np.matmul(h1.T, delta_2) / batch_size # shape: (隠れ層の次元数, 出力層の次元数)\n",
    "    db2 = np.matmul(np.ones(batch_size), delta_2) / batch_size # shape: (出力層の次元数,)\n",
    "\n",
    "    # パラメータの更新\n",
    "    W1 -= eps * dW1\n",
    "    b1 -= eps * db1\n",
    "    \n",
    "    W2 -= eps * dW2\n",
    "    b2 -= eps * db2\n",
    "\n",
    "    return cost\n",
    "\n",
    "def valid_xor(x, t):\n",
    "    global W1, b1, W2, b2\n",
    "    \n",
    "    # 順伝播\n",
    "    u1 = np.matmul(x, W1) + b1\n",
    "    h1 = relu(u1)\n",
    "    \n",
    "    # 逆伝播\n",
    "    u2 = np.matmul(h1, W2) + b2\n",
    "    y = sigmoid(u2)\n",
    "    \n",
    "    # 誤差の計算\n",
    "    cost = (- t * np_log(y) - (1 - t) * np_log(1 - y)).mean()\n",
    "    \n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99905289]\n",
      " [0.99902941]\n",
      " [0.00823191]\n",
      " [0.00103141]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3000):\n",
    "    # オンライン学習\n",
    "    for x, t in zip(x_train_xor, t_train_xor):\n",
    "        cost = train_xor(x[None, :], t[None, :], eps=0.05)\n",
    "\n",
    "cost, y_pred = valid_xor(x_valid_xor, t_valid_xor)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題 2. 多層パーセプトロンの実装と学習 (MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ソフトマックス関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "    \\mathrm{softmax}({\\bf x})_k = \\frac{\\exp({\\bf x}_k)}{\\sum^K_{k'=1}\\exp({\\bf x}_{k'})} \\hspace{10mm} \\text{for} \\hspace{3mm} k = 1, \\ldots, K\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathrm{softmax}'({\\bf x})_k = \\mathrm{softmax}({\\bf x})_k (1 - \\mathrm{softmax}({\\bf x})_k)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x -= x.max(axis=1, keepdims=True)\n",
    "    x_exp = np.exp(x)\n",
    "    return x_exp / np.sum(x_exp, axis=1, keepdims=True)\n",
    "\n",
    "def deriv_softmax(x):\n",
    "    return softmax() * (1 - softmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. データセットの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = fetch_mldata('MNIST original')\n",
    "mnist = fetch_openml('mnist_784') # 上の行でHTTPErrorが出る場合は代わりにこちらを実行してください\n",
    "\n",
    "x_mnist = mnist.data.astype('float32') / 255.\n",
    "t_mnist = np.eye(10)[mnist.target.astype('int32')]\n",
    "\n",
    "x_train_mnist, x_test_mnist, t_train_mnist, t_test_mnist = train_test_split(x_mnist, t_mnist, test_size=10000)\n",
    "x_train_mnist, x_valid_mnist, t_train_mnist, t_valid_mnist = train_test_split(x_train_mnist, t_train_mnist, test_size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 全結合層の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多層のMLPを実装していけるよう、全結合層をクラスとして定義していきます。\n",
    "\n",
    "順伝播、逆伝播、勾配の計算をそれぞれ関数として実装します。\n",
    "\n",
    "数式は以下のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 順伝播 (```__call__```)\n",
    "\\begin{align*}\n",
    "    {\\bf u}^{(l)} &= {\\bf W}^{(l)\\mathrm{T}} {\\bf h}^{(l-1)} + {\\bf b}^{(l)} \\\\\n",
    "    {\\bf h}^{(l)} &= \\mathrm{function}({\\bf u}^{(l)})\n",
    "\\end{align*}\n",
    "\n",
    "#### 逆伝播 (```b_prop```)\n",
    "\\begin{align*}\n",
    "    \\delta^{(l)} = \\mathrm{function}'({\\bf u}^{(l)}) \\odot ({\\bf W}^{(l+1)\\mathrm{T}} \\delta^{(l+1)})\n",
    "\\end{align*}\n",
    "\n",
    "#### 勾配の計算 (```compute_grad```)\n",
    "\\begin{align*}\n",
    "    \\nabla_{{\\bf W}^{(l)}}E &= \\frac{1}{N}\\delta^{(l)} {\\bf h}^{(l)\\mathrm{T}} \\\\\n",
    "    \\nabla_{{\\bf b}^{(l)}}E &= \\frac{1}{N}\\delta^{(l)} \\mathbb{1}_N \\\\\n",
    "\\end{align*}\n",
    "\n",
    "`get_params`、`set_params`、`get_grads`はそれぞれ重み、勾配をベクトルで受け渡す関数です。\n",
    "課題3の勾配チェックの際に使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, in_dim, out_dim, function, deriv_function):\n",
    "        self.W = np.random.uniform(low=-0.08, high=0.08,\n",
    "                                   size=(in_dim, out_dim)).astype('float64')\n",
    "        self.b = np.zeros(out_dim).astype('float64')\n",
    "        self.function = function\n",
    "        self.deriv_function = deriv_function\n",
    "        \n",
    "        self.x = None\n",
    "        self.u = None\n",
    "        \n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "        self.params_idxs = np.cumsum([self.W.size, self.b.size])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.x = x\n",
    "        self.u = np.matmul(self.x, self.W) + self.b\n",
    "        return self.function(self.u)\n",
    "\n",
    "    def b_prop(self, delta, W):\n",
    "        self.delta = self.deriv_function(self.u) * np.matmul(delta, W.T)\n",
    "        return self.delta\n",
    "    \n",
    "    def compute_grad(self):\n",
    "        batch_size = self.delta.shape[0]\n",
    "        \n",
    "        self.dW = np.matmul(self.x.T, self.delta) / batch_size\n",
    "        self.db = np.matmul(np.ones(batch_size), self.delta) / batch_size\n",
    "\n",
    "    def get_params(self):\n",
    "        return np.concatenate([self.W.ravel(), self.b], axis=0)\n",
    "    \n",
    "    def set_params(self, params):\n",
    "        _W, _b = np.split(params, self.params_idxs)[:-1]\n",
    "        self.W = _W.reshape(self.W.shape)\n",
    "        self.b = _b\n",
    "    \n",
    "    def get_grads(self):\n",
    "        return np.concatenate([self.dW.ravel(), self.db], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP全体の順伝播・逆伝播を行う関数をそれぞれ実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_props(layers, x):\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_props(layers, delta):\n",
    "    batch_size = delta.shape[0]\n",
    "    \n",
    "    for i, layer in enumerate(layers[::-1]):\n",
    "        if i == 0: # 出力層の場合\n",
    "            layer.delta = delta # y - t\n",
    "            layer.compute_grad() # 勾配の計算\n",
    "        else: # 出力層以外の場合\n",
    "            delta = layer.b_prop(delta, W) # 逆伝播\n",
    "            layer.compute_grad() # 勾配の計算\n",
    "\n",
    "        W = layer.W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータの更新を行う関数を実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(layers, eps):\n",
    "    for layer in layers:\n",
    "        layer.W -= eps * layer.dW\n",
    "        layer.b -= eps * layer.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後にMLPを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    Dense(784, 100, relu, deriv_relu),\n",
    "    Dense(100, 100, relu, deriv_relu),\n",
    "    Dense(100, 10, softmax, deriv_softmax)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. train関数とvalid関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 誤差関数\n",
    "\n",
    "負の対数尤度 (多クラス交差エントロピー)\n",
    "\n",
    "\\begin{align*}\n",
    "    E({\\bf x}, {\\bf t}) = -\\frac{1}{N}\\sum^N_{i=1}\\sum^K_{k=1} {\\bf t}_{i, k}\\log{\\bf y}_{i, k}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mst(x, t, eps=0.01):\n",
    "    # 順伝播\n",
    "    y = f_props(layers, x)\n",
    "\n",
    "    # 誤差の計算\n",
    "    cost = (- t * np_log(y)).sum(axis=1).mean()\n",
    "    \n",
    "    # 逆伝播\n",
    "    delta = y - t\n",
    "    b_props(layers, delta)\n",
    "\n",
    "    # パラメータの更新\n",
    "    update_params(layers, eps)\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_mst(x, t):\n",
    "    # 順伝播\n",
    "    y = f_props(layers, x)\n",
    "    \n",
    "    # 誤差の計算\n",
    "    cost = (- t * np_log(y)).sum(axis=1).mean()\n",
    "    \n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Valid Cost: 0.156, Valid Accuracy: 0.952\n",
      "EPOCH: 2, Valid Cost: 0.098, Valid Accuracy: 0.971\n",
      "EPOCH: 3, Valid Cost: 0.105, Valid Accuracy: 0.971\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    x_train_mnist, t_train_mnist = shuffle(x_train_mnist, t_train_mnist)\n",
    "    # オンライン学習\n",
    "    for x, t in zip(x_train_mnist, t_train_mnist):\n",
    "        cost = train_mst(x[None, :], t[None, :], eps=0.01)\n",
    "    \n",
    "    cost, y_pred = valid_mst(x_valid_mnist, t_valid_mnist)\n",
    "    accuracy = accuracy_score(t_valid_mnist.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    print('EPOCH: {}, Valid Cost: {:.3f}, Valid Accuracy: {:.3f}'.format(epoch + 1, cost, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題 3. 数値微分 (勾配チェック)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差逆伝播法による勾配の計算は少し複雑なため、実装にバグが入りがちです。\n",
    "\n",
    "実装が簡単な数値微分と結果を比較することで、逆伝播の実装が正しいかを確認してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 1変数の場合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず肩慣らしに簡単な2次関数に対して数値微分を行ってみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def deriv_f(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1変数の場合の数値微分の式は次のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    f'(x) = \\lim_{h \\rightarrow 0} \\frac{f(x + h) - f(x - h)}{2h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 4.000000000026205)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 1e-5\n",
    "x = 2.0\n",
    "\n",
    "grad_auto = deriv_f(x)\n",
    "grad_num = (f(x + eps) - f(x - eps)) / (2 * eps)\n",
    "\n",
    "grad_auto, grad_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 多変数の場合 (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に課題2で定義したMLPに対して数値微分の計算を行い、誤差逆伝播による勾配 (`dW`、`db`) の計算が間違っていないかを確認してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "多変数 (MLP) の場合の数値微分の式は次のようになります。ある1つの変数$\\theta_m$のみを$h$だけ動かした場合を考えます。\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial E}{\\partial \\theta_m} = \\lim_{h\\rightarrow 0} \\frac{E(\\theta_1, \\theta_2, \\ldots, \\theta_m + h, \\ldots, \\theta_M) - E(\\theta_1, \\theta_2, \\ldots, \\theta_m - h, \\ldots, \\theta_M)}{2h}\n",
    "$$\n",
    "\n",
    "実装では、変数全体のサイズのゼロベクトルを用意し、$m$番目の要素のみ$h$だけずらされたベクトルを作りそれに対応する誤差を計算し、そこから上の式に従って最終的な微分の値を求めていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず各層ごとの重みをベクトルで取得しリストで保存していく関数を実装していきます。\n",
    "\n",
    "MLPの各レイヤーから重みをベクトルで取得するために、先程`Dense`クラスで定義した`set_params`、`get_params`を使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_params(layers):\n",
    "    params_all = []\n",
    "    for layer in layers:\n",
    "        params = layer.get_params()\n",
    "        params_all.append(params)\n",
    "    \n",
    "    return params_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_params(layers, params_all):\n",
    "    for layer, params in zip(layers, params_all):\n",
    "        layer.set_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(x, t):\n",
    "    # 順伝播\n",
    "    y = f_props(layers, x)\n",
    "    \n",
    "    # 誤差の計算\n",
    "    cost = (- t * np_log(y)).sum(axis=1).mean()\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "勾配の計算に使用するデータを用意します。勾配チェックの際は`float32`ではなく`float64`を使いましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "x = x_train_mnist[:batch_size].astype('float64')\n",
    "t = t_train_mnist[:batch_size].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "\n",
    "params_all = get_params(layers)\n",
    "grads_all_num = []\n",
    "\n",
    "# レイヤーごとに勾配を計算\n",
    "for layer, params in zip(layers, params_all):\n",
    "    shift = np.zeros_like(params)\n",
    "    grads_num = np.zeros_like(params)\n",
    "    \n",
    "    # レイヤー内のM個のパラメータに対してそれぞれ微分を計算する\n",
    "    for m in range(len(params)):\n",
    "        shift[m] = eps # m番目のパラメータのみeps分ずらす [0, 0, ..., 0, eps, 0, ..., 0]\n",
    "        \n",
    "        params_right = params + shift\n",
    "        layer.set_params(params_right)\n",
    "        cost_right = compute_cost(x, t) # L(x; ..., \\theta_m + eps, ...)\n",
    "        \n",
    "        params_left = params - shift\n",
    "        layer.set_params(params_left)\n",
    "        cost_left = compute_cost(x, t) # L(x; ..., \\theta_m - eps, ...)\n",
    "        \n",
    "        grads_num[m] = (cost_right - cost_left) / (2 * eps) # 微分の計算\n",
    "        \n",
    "        layer.set_params(params) # パラメータをもとに戻す\n",
    "        shift[m] = 0\n",
    "    \n",
    "    grads_all_num.append(grads_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. 誤差逆伝播法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grads(layers):\n",
    "    grads_all = []\n",
    "    for layer in layers:\n",
    "        grads = layer.get_grads()\n",
    "        grads_all.append(grads)\n",
    "    \n",
    "    return grads_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 順伝播\n",
    "y = f_props(layers, x)\n",
    "\n",
    "# 逆伝播\n",
    "delta = y - t\n",
    "b_props(layers, delta)\n",
    "\n",
    "# 勾配を取得\n",
    "grads_all_bprop = get_grads(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. 比較 (勾配チェック)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差逆伝播法で計算した勾配と数値微分による勾配の差を、ノルムで正規化したrelative differenceで測ります。[1]\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathrm{diff} = \\frac{||\\mathrm{grad}_{\\mathrm{bprop}} - \\mathrm{grad}_{\\mathrm{num}}||_2}{||\\mathrm{grad}_{\\mathrm{bprop}}||_2 + ||\\mathrm{grad}_{\\mathrm{num}}||_2}\n",
    "\\end{align*}\n",
    "\n",
    "経験的にはその差がおおよそ$1e-7$以下であれば実装にバグはないと安心して良いでしょう。[1]\n",
    "\n",
    "[1] Improving Deep Neural Networks: Gradient checking: https://www.coursera.org/lecture/deep-neural-network/gradient-checking-htA0l (2018年10月17日閲覧)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients' difference (layer 1): 1.7616233872314648e-10\n",
      "Gradients' difference (layer 2): 1.1121350841796051e-10\n",
      "Gradients' difference (layer 3): 6.973186982608638e-11\n"
     ]
    }
   ],
   "source": [
    "for i, (grads_bprop, grads_num) in enumerate(zip(grads_all_bprop, grads_all_num)):\n",
    "    \n",
    "    diff = np.linalg.norm(grads_bprop - grads_num) / (np.linalg.norm(grads_bprop) + np.linalg.norm(grads_num))\n",
    "    \n",
    "    print('Gradients\\' difference (layer {}): {}'.format(i + 1, diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
