{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第2回講義 演習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import fetch_mldata, fetch_openml\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目次\n",
    "\n",
    "課題1. ロジスティック回帰の実装と学習 (OR)\n",
    "1. シグモイド関数\n",
    "2. データセットの設定と重みの定義\n",
    "3. train関数とvalid関数\n",
    "4. 学習\n",
    "\n",
    "課題2. ソフトマックス回帰の実装と学習 (MNIST)\n",
    "1. ソフトマックス関数\n",
    "2. データセットの設定と重みの定義\n",
    "3. train関数とvalid関数\n",
    "4. 学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題1. ロジスティック回帰の実装と学習 (OR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. シグモイド関数\n",
    "$$\n",
    "    \\sigma({\\bf x}) = \\frac{1}{1 + \\exp(-{\\bf x})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "#     return 1 / (1 + np.exp(- x))\n",
    "    return np.tanh(x * 0.5) * 0.5 + 0.5 # numpy組み込みのtanhを利用 (expのoverflowを防ぐ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. データセットの設定と重みの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORのデータセット\n",
    "x_train_or = np.array([[0, 1], [1, 0], [0, 0], [1, 1]])\n",
    "t_train_or = np.array([[1], [1], [0], [1]])\n",
    "x_valid_or, t_valid_or = x_train_or, t_train_or\n",
    "x_test_or, t_test_or = x_train_or, t_train_or\n",
    "\n",
    "# 重み (入力の次元数: 2, 出力の次元数: 1)\n",
    "W_or = np.random.uniform(low=-0.08, high=0.08, size=(2, 1)).astype('float32')\n",
    "b_or = np.zeros(shape=(1,)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. train関数とvalid関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. 誤差関数\n",
    "- 負の対数尤度 (交差エントロピー)\n",
    "\n",
    "$$ E ({\\bf x}, {\\bf t}; {\\bf W}, {\\bf b} ) =  -\\frac{1}{N}\\sum^N_{i=1} \\left[ {\\bf t}_i \\log {\\bf y}_i ({\\bf x}_i; {\\bf W}, {\\bf b}) + (1 - {\\bf t}_i) \\log \\{ 1 - {\\bf y}_i ({\\bf x}_i; {\\bf W}, {\\bf b}) \\}\\right] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. 順伝播\n",
    "$$\n",
    "    {\\bf y} = \\sigma({\\bf W}^{\\mathrm{T}} {\\bf x} + {\\bf b})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. 逆伝播\n",
    "\\begin{align*}\n",
    "    \\delta &= {\\bf y} - {\\bf t} \\\\\n",
    "    \\nabla_{\\bf W} E &= \\frac{1}{N}\\delta {\\bf x}^{\\mathrm{T}} \\\\\n",
    "    \\nabla_{\\bf b} E &= \\frac{1}{N}\\delta \\mathbb{1}_N \\\\\n",
    "    {\\bf W} &\\leftarrow {\\bf W} - \\epsilon \\nabla_{\\bf W} E \\\\\n",
    "    {\\bf b} &\\leftarrow {\\bf b} - \\epsilon \\nabla_{\\bf b} E \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logの中身が0になるのを防ぐ\n",
    "def np_log(x):\n",
    "    return np.log(np.clip(a=x, a_min=1e-10, a_max=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_or(x, t, eps=1.0):\n",
    "    \"\"\"\n",
    "    :param x: np.ndarray, 入力データ, shape=(batch_size, 入力の次元数)\n",
    "    :param t: np.ndarray, 教師ラベル, shape=(batch_size, 出力の次元数)\n",
    "    :param eps: float, 学習率\n",
    "    \"\"\"\n",
    "    global W_or, b_or\n",
    "    \n",
    "    batch_size = x.shape[0]\n",
    "    \n",
    "    # 順伝播\n",
    "    y = sigmoid(np.matmul(x, W_or) + b_or) # shape: (batch_size, 出力の次元数)\n",
    "    \n",
    "    # 逆伝播\n",
    "    cost = (- t * np_log(y) - (1 - t) * np_log(1 - y)).mean()\n",
    "    delta = y - t # shape: (batch_size, 出力の次元数)\n",
    "    \n",
    "    # パラメータの更新\n",
    "    dW = np.matmul(x.T, delta) / batch_size # shape: (入力の次元数, 出力の次元数)\n",
    "    db = np.matmul(np.ones(shape=(batch_size,)), delta) / batch_size # shape: (出力の次元数,)\n",
    "    W_or -= eps * dW\n",
    "    b_or -= eps * db\n",
    "\n",
    "    return cost\n",
    "\n",
    "def valid_or(x, t):\n",
    "    y = sigmoid(np.matmul(x, W_or) + b_or)\n",
    "    cost = (- t * np_log(y) - (1 - t) * np_log(1 - y)).mean()\n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9979988 ]\n",
      " [0.99799904]\n",
      " [0.00500219]\n",
      " [0.99999998]]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    # オンライン学習\n",
    "    x_train_or, t_train_or = shuffle(x_train_or, t_train_or)\n",
    "    for x, t in zip(x_train_or, t_train_or):\n",
    "        cost = train_or(x[None, :], t[None, :])\n",
    "    cost, y_pred = valid_or(x_valid_or, t_valid_or)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題2. ソフトマックス回帰の実装と学習 (MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ソフトマックス関数\n",
    "$$\n",
    "    \\mathrm{softmax}({\\bf x})_k = \\frac{\\exp({\\bf x}_k)}{\\sum^K_{k'=1} \\exp({\\bf x}_{k'})} \\hspace{10mm} \\text{for} \\, k=1,\\ldots, K\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x -= x.max(axis=1, keepdims=True) # expのunderflow & overflowを防ぐ\n",
    "    x_exp = np.exp(x)\n",
    "    return x_exp / np.sum(x_exp, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. データセットの設定と重みの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mnist = fetch_mldata(dataname='MNIST original')\n",
    "mnist = fetch_openml(name='mnist_784') # 上の行でHTTPErrorが出る場合は代わりにこちらを実行してください (少し時間がかかります)\n",
    "\n",
    "x_mnist = mnist.data.astype('float32') / 255.\n",
    "t_mnist = np.eye(N=10)[mnist.target.astype('int32')]\n",
    "\n",
    "x_train_mnist, x_test_mnist, t_train_mnist, t_test_mnist = train_test_split(x_mnist, t_mnist, test_size=10000)\n",
    "x_train_mnist, x_valid_mnist, t_train_mnist, t_valid_mnist = train_test_split(x_train_mnist, t_train_mnist, test_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重み (入力の次元数: 784, 出力の次元数: 10)\n",
    "W_mnist = np.random.uniform(low=-0.08, high=0.08, size=(784, 10)).astype('float32')\n",
    "b_mnist = np.zeros(shape=(10,)).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. train関数とvalid関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. 誤差関数\n",
    "- 負の対数尤度 (多クラス交差エントロピー)\n",
    "$$ E ({\\bf x}, {\\bf t}; {\\bf W}, {\\bf b} ) =  -\\frac{1}{N}\\sum^N_{i=1} \\sum^K_{k=1} {\\bf t}_{i, k} \\log {\\bf y}_{i, k} ({\\bf x}_i; {\\bf W}, {\\bf b}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. 順伝播\n",
    "$$\n",
    "    {\\bf y} = \\mathrm{softmax}({\\bf W}^{\\mathrm{T}}{\\bf x} + {\\bf b})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. 逆伝播\n",
    "\\begin{align*}\n",
    "    \\delta &= {\\bf y} - {\\bf t} \\\\\n",
    "    \\nabla_{\\bf W} E &= \\frac{1}{N}\\delta {\\bf x}^{\\mathrm{T}} \\\\\n",
    "    \\nabla_{\\bf b} E &= \\frac{1}{N}\\delta \\mathbb{1}_N \\\\\n",
    "    {\\bf W} &\\leftarrow {\\bf W} - \\epsilon \\nabla_{\\bf W} E \\\\\n",
    "    {\\bf b} &\\leftarrow {\\bf b} - \\epsilon \\nabla_{\\bf b} E \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(x, t, eps=1.0):\n",
    "    \"\"\"\n",
    "    :param x: np.ndarray, 入力データ, shape=(batch_size, 入力の次元数)\n",
    "    :param t: np.ndarray, 教師ラベル, shape=(batch_size, 出力の次元数)\n",
    "    :param eps: float, 学習率\n",
    "    \"\"\"\n",
    "    global W_mnist, b_mnist\n",
    "    \n",
    "    batch_size = x.shape[0]\n",
    "    \n",
    "    # 順伝播\n",
    "    y = softmax(np.matmul(x, W_mnist) + b_mnist) # shape: (batch_size, 出力の次元数)\n",
    "    \n",
    "    # 逆伝播\n",
    "    cost = (- t * np_log(y)).sum(axis=1).mean()\n",
    "    delta = y - t # shape: (batch_size, 出力の次元数)\n",
    "    \n",
    "    # パラメータの更新\n",
    "    dW = np.matmul(x.T, delta) / batch_size # shape: (入力の次元数, 出力の次元数)\n",
    "    db = np.matmul(np.ones(shape=(batch_size,)), delta) / batch_size # shape: (出力の次元数,)\n",
    "    W_mnist -= eps * dW\n",
    "    b_mnist -= eps * db\n",
    "\n",
    "    return cost\n",
    "\n",
    "def valid_mnist(x, t):\n",
    "    y = softmax(np.matmul(x, W_mnist) + b_mnist)\n",
    "    cost = (- t * np_log(y)).sum(axis=1).mean()\n",
    "    \n",
    "    return cost, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Valid Cost: 2.649, Valid Accuracy: 0.866\n",
      "EPOCH: 2, Valid Cost: 2.224, Valid Accuracy: 0.887\n",
      "EPOCH: 3, Valid Cost: 2.220, Valid Accuracy: 0.888\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    # オンライン学習\n",
    "    x_train_mnist, t_train_mnist = shuffle(x_train_mnist, t_train_mnist)\n",
    "    for x, t in zip(x_train_mnist, t_train_mnist):\n",
    "        cost = train_mnist(x[None, :], t[None, :])\n",
    "    cost, y_pred = valid_mnist(x_valid_mnist, t_valid_mnist)\n",
    "    print('EPOCH: {}, Valid Cost: {:.3f}, Valid Accuracy: {:.3f}'.format(\n",
    "        epoch + 1,\n",
    "        cost,\n",
    "        accuracy_score(t_valid_mnist.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
